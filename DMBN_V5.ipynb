{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32425/3794690065.py:15: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display as html_width\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "from keras.layers import Add,Multiply,Softmax,Input,TimeDistributed,Dense,Average,GlobalAveragePooling1D,Concatenate,Lambda,RepeatVector, Conv2D,ConvLSTM2D, MaxPooling2D,BatchNormalization,Flatten,Reshape,UpSampling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.display import display as html_width\n",
    "html_width(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow_addons as tfa\n",
    "import psutil\n",
    "import pickle\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n",
      "2.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 13:13:17.721900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-20 13:13:17.721997: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 28 36 49 24 13  0 25 41 17 16  7 21 29 12  3  4 40 23 34 45 44 42 26\n",
      " 48 18 27  9 33  1 14 35 47 46 11 43 39  6  8  2 19 15 31 22  5 38 10 32\n",
      " 30 20]\n",
      "Remaining train_p: [41, 7, 21, 29, 12, 3, 4, 40, 23, 34, 45, 44, 42, 26, 48, 18, 27, 9, 33, 1, 14, 35, 47, 46, 11, 43, 39, 6, 8, 2, 19, 15, 31, 22, 5, 38, 10, 32, 30, 20]\n",
      "test_p: [37, 28, 36, 49, 24, 13, 0, 25, 17, 16]\n"
     ]
    }
   ],
   "source": [
    "obs_min = 4\n",
    "obs_max = 8\n",
    "train_N = 49\n",
    "train_p = np.random.permutation(50)\n",
    "j = 0\n",
    "x = 0\n",
    "test_p = []\n",
    "valid_p = []\n",
    "print(train_p)\n",
    "\n",
    "train_p_copy = train_p.copy()\n",
    "\n",
    "for i in train_p_copy:\n",
    "    if i <= 24 and j < 5:\n",
    "        test_p.append(i)\n",
    "        j += 1\n",
    "        train_p = [item for item in train_p if item != i]  # Remove from train_p\n",
    "    if i > 24 and x < 5:\n",
    "        test_p.append(i)\n",
    "        x += 1\n",
    "        train_p = [item for item in train_p if item != i]  # Remove from train_p\n",
    "\n",
    "print(\"Remaining train_p:\", train_p)\n",
    "print(\"test_p:\", test_p)\n",
    "\n",
    "# j = 0\n",
    "# x = 0\n",
    "# test_p_copy = test_p.copy()\n",
    "# for i in test_p_copy:\n",
    "#     if i < 25 and j < 3:\n",
    "#         valid_p.append(i)\n",
    "#         j += 1\n",
    "#         test_p = [item for item in test_p if item != i]\n",
    "#     if i > 25 and x < 2:\n",
    "#         valid_p.append(i)\n",
    "#         x += 1\n",
    "#         test_p = [item for item in test_p if item != i] \n",
    "# print(\"valid_p:\", valid_p)\n",
    "# print(\"test_p:\", test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, aug_prob=0.7):\n",
    "    aug_image = image.copy()\n",
    "\n",
    "    if np.random.rand() < aug_prob:\n",
    "\n",
    "        if np.random.rand() < 0.35:\n",
    "            crop_factor = np.random.uniform(0.5, 0.8) \n",
    "            aug_image = tf.image.central_crop(aug_image, crop_factor)\n",
    "\n",
    "        aug_image = tf.image.resize(aug_image, (128, 128))\n",
    "\n",
    "        if np.random.rand() < 0.35:\n",
    "            aug_image = tf.image.random_brightness(aug_image, max_delta=0.5) \n",
    "\n",
    "        if np.random.rand() < 0.35:\n",
    "            angles = tf.random.uniform([], minval=-0.1, maxval=0.1) \n",
    "            aug_image = tfa.image.rotate(aug_image, angles)\n",
    "\n",
    "        if np.random.rand() < 0.35:\n",
    "            translations = tf.random.uniform([2], minval=-10, maxval=10)\n",
    "            aug_image = tfa.image.translate(aug_image, translations)\n",
    "\n",
    "        # if np.random.rand() < 0.35:\n",
    "        #     aug_image = tf.image.flip_left_right(aug_image)\n",
    "            \n",
    "    else:\n",
    "        aug_image = tf.image.resize(aug_image, (128, 128))\n",
    "\n",
    "    return aug_image\n",
    "\n",
    "def augment_touch_values(touch_values):\n",
    "    if np.random.rand() < 0.7:  \n",
    "        noise = np.random.normal(0, 0.002, size=touch_values.shape)  \n",
    "        touch_values = np.clip(touch_values + noise, 0, 1)  \n",
    "        touch_values[touch_values == 0] = 0  \n",
    "    return touch_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Modality Blending Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5333 0.4235 0.2039 0.1686 0.5098 0.3804 0.2000 0.1490]\n",
      "Cona\n",
      "[0.5475 0.4228 0.1128 0.2034 0.4435 0.4416 0.2232 0.1441]\n",
      "[0.4157 0.3882 0.3569 0.3922 0.3686 0.3176 0.3294 0.3294]\n",
      "Cona\n",
      "[0.4904 0.3381 0.4489 0.3794 0.2871 0.1851 0.3532 0.2356]\n",
      "[0.2510 0.2745 0.4039 0.4980 0.2157 0.2314 0.3804 0.4431]\n",
      "Cona\n",
      "[0.2510 0.2745 0.4039 0.4980 0.2157 0.2314 0.3804 0.4431]\n",
      "[0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]\n",
      "Cona\n",
      "[0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000]\n",
      "[0.3255 0.3098 0.3216 0.3686 0.3098 0.2902 0.3176 0.3412]\n",
      "Cona\n",
      "[0.3255 0.3098 0.3216 0.3686 0.3098 0.2902 0.3176 0.3412]\n",
      "[0.4000 0.3490 0.3765 0.4078 0.3843 0.3882 0.4000 0.4431]\n",
      "Cona\n",
      "[0.4000 0.3490 0.3765 0.4078 0.3843 0.3882 0.4000 0.4431]\n",
      "[0.0039 0.0078 0.0353 0.0706 0.0000 0.0000 0.0353 0.0549]\n",
      "Cona\n",
      "[0.0163 0.0000 0.1089 0.0000 0.0410 0.0264 0.0637 0.0560]\n"
     ]
    }
   ],
   "source": [
    "######################################################################### GET TRAIN SAMPLES ############################################################################\n",
    "\n",
    "def get_train_sample(coef = -1):\n",
    "    n = np.random.randint(obs_min, obs_max) + 1\n",
    "    d = train_p[np.random.randint(0, 39)]\n",
    "    \n",
    "    if coef == -1:\n",
    "         coef = np.random.uniform(0.2, 1.0)\n",
    "    \n",
    "    img_coef = np.ones((1, 128)) * coef\n",
    "    touch_coef = np.ones((1, 128)) * (1 - coef)\n",
    "    \n",
    "    observation = np.zeros((1, n, 128, 128, 4)) \n",
    "    observation_touch = np.zeros((1, n, 9)) \n",
    "    target_X = np.zeros((1, 1))\n",
    "    target_Y = np.zeros((1, 128, 128, 6))\n",
    "    #target_Y_touch = np.zeros((1, 16))\n",
    "    target_Y_touch = np.zeros((1, 8))\n",
    "    \n",
    "    pose = np.loadtxt('Data_Robot/%d/touch_data.txt' % d)\n",
    "    time_len = pose.shape[0]\n",
    "    times = np.linspace(0, 1, time_len)\n",
    "\n",
    "    selected_indices = np.random.permutation(time_len)\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        if i < n -1:\n",
    "            observation[0, i, :, :, 0] = np.ones((128, 128)) * times[idx]\n",
    "            image = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, idx))\n",
    "            #image_resized = cv2.resize(image, (128, 128))\n",
    "            augmented_image = augment_image(image)\n",
    "            observation[0, i, :, :, 1:] = augmented_image \n",
    "\n",
    "            # display.clear_output(wait=True)\n",
    "            # plt.imshow(observation[0, i, :, :, :3])\n",
    "            # plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "            #print(observation[0, i, :, :, 1:])\n",
    "\n",
    "            observation_touch[0, i, 0] = times[idx]\n",
    "            observation_touch[0, i, 1:9] = augment_touch_values(pose[idx] / 255)\n",
    "            # print(pose[idx] / 255)\n",
    "            # print(\"Cona\")\n",
    "            # print(observation_touch[0, i, 1:9])\n",
    "            \n",
    "\n",
    "\n",
    "    target_X[0, 0] = times[selected_indices[-1]]\n",
    "    #print(target_X)\n",
    "    t_img = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, selected_indices[-1]))\n",
    "    t_img_resize = cv2.resize(t_img, (128, 128))\n",
    "    target_Y[0, :, :, :3] = t_img_resize \n",
    "    target_Y_touch[0, :8] = pose[selected_indices[-1]] / 255\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(pl.gcf())\n",
    "    # plt.imshow(target_Y[0,:,:,:3])\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    #print (target_Y)\n",
    "\n",
    "    return [observation, observation_touch, target_X, img_coef, touch_coef], [target_Y, target_Y_touch], d, selected_indices[-1], coef\n",
    "\n",
    "inp, out, _, _, _ = get_train_sample()\n",
    "# print(out)\n",
    "\n",
    "######################################################################### GET VALIDATION SAMPLES ############################################################################\n",
    "\n",
    "\n",
    "def get_valid_sample(): \n",
    "    n = np.random.randint(obs_min, obs_max) + 1\n",
    "    d = valid_p[np.random.randint(0, 4)]\n",
    "\n",
    "    coef = 1\n",
    "    \n",
    "    img_coef = np.ones((1, 128)) * coef\n",
    "    touch_coef = np.ones((1, 128)) * (1 - coef)\n",
    "    \n",
    "    observation = np.zeros((1, n, 128, 128, 4)) \n",
    "    observation_touch = np.zeros((1, n, 9)) \n",
    "    target_X = np.zeros((1, 1))\n",
    "    target_Y = np.zeros((1, 128, 128, 6))\n",
    "    #target_Y_touch = np.zeros((1, 16))\n",
    "    target_Y_touch = np.zeros((1, 8))\n",
    "    \n",
    "    pose = np.loadtxt('Data_Robot/%d/touch_data.txt' % d)\n",
    "    time_len = pose.shape[0]\n",
    "    times = np.linspace(0, 1, time_len)\n",
    "\n",
    "    # Identify zero and nonzero rows\n",
    "    zero_rows = np.where(np.all(pose == 0, axis=1))[0]\n",
    "    nonzero_rows = np.where(np.any(pose != 0, axis=1))[0]\n",
    "    \n",
    "    if len(nonzero_rows) == 0 or len(zero_rows) == 0:\n",
    "        raise ValueError(\"Dataset does not contain both zero and nonzero rows.\")\n",
    "\n",
    "    # Ensure at least one sample from each\n",
    "    num_from_each = min(n // 2, len(nonzero_rows), len(zero_rows))\n",
    "    selected_nonzero = np.random.choice(nonzero_rows, num_from_each, replace=False)\n",
    "    selected_zero = np.random.choice(zero_rows, num_from_each, replace=False)\n",
    "\n",
    "    remaining_samples = n - 2 * num_from_each\n",
    "    if remaining_samples > 0:\n",
    "        additional_samples = np.random.choice(time_len, remaining_samples, replace=False)\n",
    "    else:\n",
    "        additional_samples = np.array([], dtype=int)\n",
    "    selected_indices = np.concatenate((selected_nonzero, selected_zero, additional_samples))\n",
    "    np.random.shuffle(selected_indices)\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        if i < n -1:\n",
    "            observation[0, i, :, :, 0] = np.ones((128, 128)) * times[idx]\n",
    "            image = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, idx))\n",
    "            image_resized = cv2.resize(image, (128, 128))\n",
    "            observation[0, i, :, :, 1:] = image_resized / 255\n",
    "\n",
    "            observation_touch[0, i, 0] = times[idx]\n",
    "            observation_touch[0, i, 1:9] = pose[idx] / 255\n",
    "\n",
    "    target_X[0, 0] = times[selected_indices[-1]]\n",
    "    t_img = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, selected_indices[-1]))\n",
    "    t_img_resize = cv2.resize(t_img, (128, 128))\n",
    "    target_Y[0, :, :, :3] = t_img_resize / 255.\n",
    "    target_Y_touch[0, :8] = pose[selected_indices[-1]] / 255\n",
    "\n",
    "    return [observation, observation_touch, target_X, img_coef, touch_coef], [target_Y, target_Y_touch], d, selected_indices[-1]\n",
    "\n",
    "\n",
    "########################################################################## GET TEST SAMPLES ####################################################################################\n",
    "\n",
    "\n",
    "def get_test_sample(t,n, size):\n",
    "    d = test_p[t]\n",
    "    out = []\n",
    "    #print(d)\n",
    "    coef = 1\n",
    "    img_coef = np.ones((1, 128)) * coef\n",
    "    touch_coef = np.ones((1, 128)) * (1 - coef)\n",
    "\n",
    "    observation = np.zeros((1, n, 128, 128, 4)) \n",
    "    observation_touch = np.zeros((1, n, 9)) \n",
    "    target_X = np.zeros((1, 1))\n",
    "    \n",
    "    pose = np.loadtxt('Data_Robot/%d/touch_data.txt' % d)\n",
    "    time_len = pose.shape[0]\n",
    "    times = np.linspace(0, 1, time_len)\n",
    "\n",
    "    target_Y = np.zeros((1, 128, 128, 6))\n",
    "    target_Y_touch = np.zeros((1, 8))\n",
    "\n",
    "    for idx in range(size):\n",
    "        if idx < n:\n",
    "            observation[0, idx, :, :, 0] = np.ones((128, 128)) * times[idx]\n",
    "            image = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, idx))\n",
    "            image_resized = cv2.resize(image, (128, 128))\n",
    "            observation[0, idx, :, :, 1:] = image_resized \n",
    "            target_Y[0, :, :, :3] = image_resized \n",
    "\n",
    "            observation_touch[0, idx, 0] = times[idx]\n",
    "            observation_touch[0, idx, 1:9] = pose[idx] / 255\n",
    "            target_Y_touch[0, :8] = pose[idx] / 255\n",
    "        else:\n",
    "            t_img = mpimg.imread('Data_Robot/%d/frame_%d.jpeg' % (d, idx))\n",
    "            t_img_resize = cv2.resize(t_img, (128, 128))\n",
    "            target_Y[0, :, :, :3] = t_img_resize \n",
    "            target_Y_touch[0, :8] = pose[idx] / 255\n",
    "        out.append([target_Y.copy(), target_Y_touch.copy()])\n",
    "\n",
    "    target_X[0, 0] = times[n]\n",
    "\n",
    "    return [observation, observation_touch, target_X, img_coef, touch_coef], out, idx, times\n",
    "\n",
    "#inp, out, x, clock = get_test_sample(1,5,55)\n",
    "#print(out[44][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_loss(y_true, y_predicted):\n",
    "    mean, log_sigma = tf.split(y_predicted, 2, axis=-1)\n",
    "    y_true_value, temp =tf.split(y_true,2,axis=-1)\n",
    "    #mean = tf.nn.relu(mean)\n",
    "    sigma = tf.nn.softplus(log_sigma)\n",
    "    dist = tfp.distributions.MultivariateNormalDiag(loc=mean, scale_diag=sigma)\n",
    "    loss = -tf.reduce_mean(dist.log_prob(y_true_value))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_observation (InputLa  [(None, None, 128, 128, 4)   0         []                            \n",
      " yer)                        ]                                                                    \n",
      "                                                                                                  \n",
      " time_distributed_28 (TimeD  (None, None, 128, 128, 32)   1184      ['image_observation[0][0]']   \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_29 (TimeD  (None, None, 64, 64, 32)     0         ['time_distributed_28[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_30 (TimeD  (None, None, 64, 64, 64)     18496     ['time_distributed_29[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_31 (TimeD  (None, None, 32, 32, 64)     0         ['time_distributed_30[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_32 (TimeD  (None, None, 32, 32, 64)     36928     ['time_distributed_31[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_33 (TimeD  (None, None, 16, 16, 64)     0         ['time_distributed_32[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " touch_observation (InputLa  [(None, None, 9)]            0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_34 (TimeD  (None, None, 16, 16, 128)    73856     ['time_distributed_33[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_21 (TimeD  (None, None, 32)             320       ['touch_observation[0][0]']   \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_35 (TimeD  (None, None, 8, 8, 128)      0         ['time_distributed_34[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_22 (TimeD  (None, None, 64)             2112      ['time_distributed_21[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_36 (TimeD  (None, None, 8, 8, 128)      147584    ['time_distributed_35[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_23 (TimeD  (None, None, 64)             4160      ['time_distributed_22[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_37 (TimeD  (None, None, 4, 4, 128)      0         ['time_distributed_36[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_24 (TimeD  (None, None, 128)            8320      ['time_distributed_23[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_38 (TimeD  (None, None, 4, 4, 256)      295168    ['time_distributed_37[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_25 (TimeD  (None, None, 128)            16512     ['time_distributed_24[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_39 (TimeD  (None, None, 2, 2, 256)      0         ['time_distributed_38[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_26 (TimeD  (None, None, 256)            33024     ['time_distributed_25[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_40 (TimeD  (None, None, 1024)           0         ['time_distributed_39[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_27 (TimeD  (None, None, 128)            32896     ['time_distributed_26[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " time_distributed_41 (TimeD  (None, None, 128)            131200    ['time_distributed_40[0][0]'] \n",
      " istributed)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 128)                  0         ['time_distributed_27[0][0]'] \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " touch_coef (InputLayer)     [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 128)                  0         ['time_distributed_41[0][0]'] \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " image_coef (InputLayer)     [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 128)                  0         ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'touch_coef[0][0]']          \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)       (None, 128)                  0         ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'image_coef[0][0]']          \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 128)                  0         ['multiply_2[0][0]',          \n",
      "                                                                     'multiply_3[0][0]']          \n",
      "                                                                                                  \n",
      " target_X (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " merged (Concatenate)        (None, 129)                  0         ['add_1[0][0]',               \n",
      "                                                                     'target_X[0][0]']            \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 1024)                 133120    ['merged[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 2, 2, 256)            0         ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 2, 2, 256)            590080    ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSamplin  (None, 4, 4, 256)            0         ['conv2d_20[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 4, 4, 128)            295040    ['up_sampling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSamplin  (None, 8, 8, 128)            0         ['conv2d_21[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 8, 8, 128)            147584    ['up_sampling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_8 (UpSamplin  (None, 16, 16, 128)          0         ['conv2d_22[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 64)           73792     ['up_sampling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_9 (UpSamplin  (None, 32, 32, 64)           0         ['conv2d_23[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 64)           36928     ['up_sampling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampli  (None, 64, 64, 64)           0         ['conv2d_24[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 64, 64, 32)           18464     ['up_sampling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 512)                  524800    ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampli  (None, 128, 128, 32)         0         ['conv2d_25[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 216)                  110808    ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 128, 128, 16)         4624      ['up_sampling2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 128)                  27776     ['dense_23[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 128, 128, 8)          1160      ['conv2d_26[0][0]']           \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 32)                   4128      ['dense_24[0][0]']            \n",
      "                                                                                                  \n",
      " last_conv2d (Conv2D)        (None, 128, 128, 6)          438       ['conv2d_27[0][0]']           \n",
      "                                                                                                  \n",
      " touch_values (Dense)        (None, 8)                    264       ['dense_25[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2770766 (10.57 MB)\n",
      "Trainable params: 2770766 (10.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_layer = Input(shape=(None,128,128,4), name=\"image_observation\") \n",
    "touch_layer = Input(shape=(None,9), name=\"touch_observation\") \n",
    "target_X_layer = Input(shape=(1,), name = 'target_X')\n",
    "img_coef_layer = Input(shape=(128,), name = 'image_coef')\n",
    "touch_coef_layer = Input(shape=(128,), name = 'touch_coef')\n",
    "\n",
    "encoder_touch_sizes = [64,64,128,128,256]\n",
    "\n",
    "touch_encoder = TimeDistributed(Dense(32, activation = 'relu'))(touch_layer)\n",
    "for channel_size in encoder_touch_sizes:\n",
    "    touch_encoder = TimeDistributed(Dense(channel_size, activation = 'relu'))(touch_encoder)\n",
    "\n",
    "touch_representations = TimeDistributed(Dense(128, activation='relu'))(touch_encoder) #128\n",
    "touch_representation = GlobalAveragePooling1D()(touch_representations) \n",
    "\n",
    "multiplied_touch = Multiply()([touch_representation,touch_coef_layer])\n",
    "\n",
    "###################################################\n",
    "\n",
    "encoder_img_sizes = [64,64,128,128,256]\n",
    "\n",
    "image_encoder = TimeDistributed(Conv2D(32,(3,3),padding='same',activation='relu'))(image_layer)\n",
    "image_encoder = TimeDistributed(MaxPooling2D((2,2)))(image_encoder)\n",
    "for channel_size in encoder_img_sizes:\n",
    "    image_encoder = TimeDistributed(Conv2D(channel_size,(3,3),padding='same',activation='relu'))(image_encoder)\n",
    "    image_encoder = TimeDistributed(MaxPooling2D((2,2)))(image_encoder)\n",
    "\n",
    "image_flatten = TimeDistributed(Flatten())(image_encoder)\n",
    "img_representations = TimeDistributed(Dense(128, activation='relu'))(image_flatten)\n",
    "img_representation = GlobalAveragePooling1D()(img_representations) \n",
    "\n",
    "multiplied_img = Multiply()([img_representation,img_coef_layer])\n",
    "\n",
    "general_representation = Add()([multiplied_touch,multiplied_img])\n",
    "\n",
    "merged_layer = Concatenate(axis=-1, name='merged')([general_representation,target_X_layer])\n",
    "\n",
    "decoder_representation = Dense(1024, activation='relu') (merged_layer)\n",
    "\n",
    "\" =============== Image Decoder =============== \"\n",
    "decoder_img = Reshape([2,2,256])(decoder_representation)\n",
    "decoder_img_sizes = [256,128,128,64,64,32]\n",
    "\n",
    "for channel_size in decoder_img_sizes:\n",
    "    decoder_img = Conv2D(channel_size, (3,3), padding='same', activation='relu')(decoder_img)\n",
    "    decoder_img = UpSampling2D((2, 2))(decoder_img)\n",
    "\n",
    "img_output = Conv2D(16, (3,3), padding='same', activation='relu')(decoder_img)\n",
    "img_output = Conv2D(8, (3,3), padding='same', activation='relu')(img_output)\n",
    "img_output = Conv2D(6, (3,3), padding='same', activation='sigmoid', name = \"last_conv2d\")(img_output)\n",
    "\" =============== Image Decoder =============== \"\n",
    "\n",
    "\" =============== Touch Decoder =============== \"\n",
    "decoder_touch = Dense(512, activation='relu')(decoder_representation)\n",
    "decoder_touch = Dense(216, activation='relu')(decoder_touch)\n",
    "decoder_touch = Dense(128, activation='relu')(decoder_touch)\n",
    "decoder_touch = Dense(32, activation='relu')(decoder_touch)\n",
    "touch_output = Dense(8, activation='sigmoid', name=\"touch_values\")(decoder_touch)\n",
    "\n",
    "\" =============== Touch Decoder =============== \"\n",
    "\n",
    "model = Model([image_layer, touch_layer, target_X_layer, img_coef_layer, touch_coef_layer],[img_output, touch_output])\n",
    "latent_model = Model([image_layer, touch_layer, target_X_layer, img_coef_layer, touch_coef_layer],general_representation)\n",
    "model.summary()\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initial_lr = 1e-4  \n",
    "# lowered_lr = 1e-5 \n",
    "\n",
    "# # Use this schedule in the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer= Adam(lr = 1e-4),\n",
    "    loss={\n",
    "        'last_conv2d': custom_loss,  \n",
    "        'touch_values': 'mse',  \n",
    "    },\n",
    "    loss_weights={\n",
    "        'last_conv2d': 1,  \n",
    "        'touch_values': 0.01, \n",
    "    },\n",
    "    metrics={  # Add accuracy tracking\n",
    "        'touch_values': ['mae'],  # Mean Absolute Error for touch values\n",
    "    }  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[273], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m inp, out, _, _, coef \u001b[38;5;241m=\u001b[39m get_train_sample()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#val_data, val_labels, _, _ = get_valid_sample()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     27\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])  \n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m loss_checkpoint \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step\u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1722\u001b[0m (\n\u001b[1;32m   1723\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_epoch,\n\u001b[1;32m   1724\u001b[0m     data_handler\u001b[38;5;241m.\u001b[39m_initial_step,\n\u001b[1;32m   1725\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[1;32m   1726\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[1;32m   1727\u001b[0m )\n\u001b[1;32m   1728\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1731\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1331\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    706\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 710\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    747\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    748\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 749\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3421\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"Epochs_V9\", exist_ok=True)\n",
    "\n",
    "loss_checkpoint = 2000\n",
    "plot_checkpoint = 1000\n",
    "validation_checkpoint = 1000\n",
    "validation_error = 9999999\n",
    "validation_step = -1\n",
    "max_training_step = 250000\n",
    "t_inp = 10\n",
    "data_size = 55\n",
    "n_test = 10\n",
    "\n",
    "dataset = ['image','touch']\n",
    "\n",
    "float_formatter = \"{:.4f}\".format\n",
    "np.set_printoptions(formatter={'float_kind': float_formatter})\n",
    "\n",
    "real_output = []\n",
    "predict_output = []\n",
    "touch_value_idx = 2\n",
    "loss_history = [] \n",
    "\n",
    "for step in range(max_training_step):\n",
    "    inp, out, _, _, coef = get_train_sample()\n",
    "    #val_data, val_labels, _, _ = get_valid_sample()\n",
    "    history = model.fit(inp, out, verbose=0) \n",
    "    loss_history.append(history.history['loss'][0])  \n",
    "\n",
    "    if step % loss_checkpoint == 0 and step!= 0:\n",
    "        path = f\"Epochs_V9/Epoch{step}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save model using pickle\n",
    "        model.save(os.path.join(path, \"model.keras\"))\n",
    "\n",
    "        # Save loss history\n",
    "        with open(os.path.join(path, \"loss_history.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "\n",
    "        for test in range(n_test):\n",
    "            step_predictions = []\n",
    "            step_real_values = []\n",
    "            step_absolute_errors_images = []\n",
    "            step_squared_errors_images = []\n",
    "            step_absolute_errors_touch = []\n",
    "            step_squared_errors_touch = []\n",
    "\n",
    "\n",
    "            test_inp, test_out, x, times = get_test_sample(test, t_inp, data_size)\n",
    "\n",
    "\n",
    "            for i in range(t_inp, data_size-1):\n",
    "                inp_copy = test_inp.copy() \n",
    "                inp_copy[2] = np.array([[times[i]]])\n",
    "                pred = model.predict_on_batch(inp_copy)\n",
    "\n",
    "                step_predictions.append(pred)\n",
    "                step_real_values.append(test_out[i])\n",
    "\n",
    "\n",
    "                pred_images = np.array(pred[0])\n",
    "                pred_touch_values = np.array(pred[1])  \n",
    "\n",
    "                out_images = np.array(test_out[i][0])  \n",
    "                out_touch_values = np.array(test_out[i][1])  \n",
    "            \n",
    "                step_absolute_errors_images.append(np.abs(out_images - pred_images))  \n",
    "                step_squared_errors_images.append((out_images - pred_images) ** 2) \n",
    "\n",
    "                step_absolute_errors_touch.append(np.abs(out_touch_values - pred_touch_values)) \n",
    "                step_squared_errors_touch.append((out_touch_values - pred_touch_values) ** 2) \n",
    "\n",
    "\n",
    "            del inp_copy, pred\n",
    "    \n",
    "            with open(os.path.join(path, f\"predictions_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_predictions, f)\n",
    "\n",
    "            with open(os.path.join(path, f\"real_values_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_real_values, f)\n",
    "    \n",
    "\n",
    "            with open(os.path.join(path, f\"absolute_errors_touch_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_absolute_errors_touch, f)\n",
    "\n",
    "            with open(os.path.join(path, f\"squared_errors_touch_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_squared_errors_touch, f)\n",
    "\n",
    "                \n",
    "            with open(os.path.join(path, f\"absolute_errors_images_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_absolute_errors_images, f)\n",
    "\n",
    "            with open(os.path.join(path, f\"squared_errors_images_{test}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(step_squared_errors_images, f)\n",
    "\n",
    "            step_predictions.clear()\n",
    "            step_real_values.clear()\n",
    "            step_absolute_errors_images.clear()\n",
    "            step_squared_errors_images.clear()\n",
    "            step_absolute_errors_touch.clear()\n",
    "            step_squared_errors_touch.clear()\n",
    "            gc.collect()\n",
    "\n",
    "    if step % 10000 == 0 and step != 0:\n",
    "        gc.collect()\n",
    "        k.clear_session()\n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
